{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym\n",
    "\n",
    "## Link utili:\n",
    "\n",
    "* Sito OpenAI: https://openai.com/\n",
    "\n",
    "* GitHub OpenAI: https://github.com/openai\n",
    "\n",
    "* Url Gym: https://gym.openai.com/\n",
    "\n",
    "* GitHub Gym: https://github.com/openai/gym\n",
    "\n",
    "* Wiki di Gym: https://github.com/openai/gym/wiki\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giochiamo a Space Invaders!\n",
    "\n",
    "![img](img/space_invaders.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo una funzione che **genera episodi** dell'ambiente passato come parametro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(env, sleep_seconds=0, get_action=None, max_steps=float('inf'), num_episodes=1,\n",
    "                   render=True, verbosity=0):\n",
    "    '''\n",
    "    Genera episodi di un ambiente gym\n",
    "    \n",
    "    Parametri\n",
    "    ---------\n",
    "    env : un ambiente gym\n",
    "    sleep_seconds : tempo di attesa in secondi tra due frame consecutivi\n",
    "    get_action : una funzione che prende un'osservazione e restituisce un'azione.\n",
    "                 Se None le azioni sono scelte casualmente\n",
    "    max_steps : numero massimo di step dell'episodio. La funziona ritorna quando\n",
    "                l'episodio è terminato o quando si è raggiunmto il numero di step\n",
    "                massimo.\n",
    "    num_episodes : numero di episodi da generare\n",
    "    render : se True visualizza gli stati dell'ambiente\n",
    "    verbosity : 0, 1, 2. Quante informazioni stampare a video\n",
    "    \n",
    "    Ritorno\n",
    "    -------\n",
    "    Una lista con le lunghezze degli episodi generati\n",
    "    '''\n",
    "    # lista contenente le lunghezze degli episodi\n",
    "    <IMPLEMENTA LA FUNZIONE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo una partita a **Space Invaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "generate_episodes(env, sleep_seconds=0.01, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune osservazioni:\n",
    "* Nei giochi **Atari** `info` (l'ultimo elemento della tupla ritornata da `step`) comunica quante **vite** sono rimaste.\n",
    "<br><br>\n",
    "* observation è un **array numpy** di dimensioni (210, 160, 3) che rappresenta un frame di gioco. Le prime due dimensioni rappresentano rispettivamente la largezza e l'altezza del frame, la terza i canali RGB.\n",
    "<br><br>\n",
    "* I **missili** sparati dalle astronavi **non sono visibili** in tutti i frame a causa delle **limitazioni hardware** dell'Atari 2600. A riguardo è possibile leggere [questo interssante articolo](https://www.wired.com/2009/03/racing-the-beam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La classe Env\n",
    "La classe più importante nell'architettura di Gym è `gym.Env`. È una classe astratta da cui ereditano tutti gli ambienti definiti nel package `gym.envs`.\n",
    "<br>\n",
    "I metodi principali della classe `Env` sono:\n",
    "* `step`: esegue un **passo** della simulazione prendendo in input un'azione. Ritorna la tupla (observation, reward, done, info) in cui **observation** è lo stato dell'ambiente dopo l'esecuzione dell'azione, **reward** è la ricompensa ricevuta, **done** vale True se l'episodio è terminato, **info** contiene informazioni ausiliarie.\n",
    "* `reset`: **inizializza** l'ambiente ritornandone il primo stato. Va chiamata **all'inizio** di ogni nuovo episodio.\n",
    "* `render`: ritorna una **rappresentazione grafica** dello stato corrente dell'ambiente. Ha le seguenti opzioni:\n",
    " * `human`: visualizza lo stato all'interno di una finestra.\n",
    " * `rgb_array`: ritorna un **array numpy** di dimensioni (larhezza, altezza, 3) che rappresenta un'immagine dello stato corrente dell'ambiente.\n",
    " * `ansi`: ritorna una **stringa** che rappresenta una visualizzazione adatta ad un **terminale** dello stato corrente.\n",
    "* `close`: esegue operazioni di **pulizia** finale.\n",
    "* `seed`: imposta il seme del generatore di **numeri causali** dell'ambiente.\n",
    "\n",
    "Il modo più semplice per **creare** un ambiente è utilizzare la funzione `gym.make` passandogli l'id dell'ambiente.\n",
    "<br>\n",
    "Si può creare un **nuovo ambiente** creando una classe che **eredita** da `Env` e ridefinendo i suoi metodi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La classe Space\n",
    "\n",
    "Ogni ambiente ha un `action_space` (con le possibili azioni) e un `observation_space` (con i possibili stati) come attributi.\n",
    "Tutti gli **spazi** sono sottoclassi di gym.Space e sono definiti nel package `gym.spaces`.\n",
    "\n",
    "I possibili spazi sono:\n",
    "* `Discrete(n)`: i suoi elementi sono i numeri interi da $0$ a $n-1$.\n",
    "* `Box`: rappresenta uno spazio continuo. Ad esempio lo spazio degli stati dei giochi Atari è di tipo Box(210, 160, 3).\n",
    "* `Multibinary`: tupla di valori binari (0,1).\n",
    "* `Multidiscrete`: Tupla di variabili discrete.\n",
    "* `Tuple`: prodotto cartesiano di spazi.\n",
    "\n",
    "I metodi principali della classe `Space` sono\n",
    "* `contains`: restituisce `True` se gli viene passato un **elemento dello spazio** come parametro.\n",
    "* `sample`: ritorna un elemento **casuale** dello spazio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizziamo lo spazio delle azioni e lo spazio degli stati di **Space Invaders**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.atari.atari_env import ACTION_MEANING\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "print(f\"action space = {env.action_space}\")\n",
    "print(f\"observation space = {env.observation_space}\")\n",
    "print(f\"observation space shape = {env.observation_space.shape}\")\n",
    "# azioni possibili e loro descrizione\n",
    "print({k : v for k, v in ACTION_MEANING.items() if k in env.action_space})\n",
    "# gli spazi continui (di tipo Box) ammettono un massimo e un minimo per ogni elemento\n",
    "print(f\"observation space lower limits = {env.observation_space.low}\")\n",
    "print(f\"observation space upper limits = {env.observation_space.high}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete\n",
    "È composto dai **numeri interi** da $0$ a $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo spazio dei numeri interi da 0 a 9\n",
    "discrete = gym.spaces.Discrete(10)\n",
    "print(f\"spazio = {discrete}\")\n",
    "print(f\"n = {discrete.n}\")\n",
    "print(f\"sample = {discrete.sample()}\")\n",
    "print(f\"contiene 10 è {discrete.contains(10)}\")\n",
    "print(f\"contiene 9 è {discrete.contains(9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box\n",
    "È uno spazio **continuo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spazio degli array numpy 2 x 3 in cui per ogni elemento vengono\n",
    "# specificati i valori minimo e massimo\n",
    "low = np.array([[-1.,-2.,-3.],[-2.,0.,-4.]])\n",
    "hi = np.array([[3.,4.,7.],[1.,10.,4.]])\n",
    "box = gym.spaces.Box(low, hi)\n",
    "print(f\"spazio = {box}\")\n",
    "print(f\"sample = {box.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multibinary\n",
    "\n",
    "I suoi elementi sono tuple di valori **binari**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spazio delle tuple binarie composte da 5 elementi\n",
    "multi_binary = gym.spaces.MultiBinary(5)\n",
    "multi_binary.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidiscrete\n",
    "\n",
    "* I suoi elementi sono **tuple di interi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terne di interi in cui il primo elemento va da 0 a 6, iil secondo da 0 a 8, il terzo da 0 a 2\n",
    "multi_discrete = gym.spaces.MultiDiscrete([7, 9, 3])\n",
    "multi_discrete.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tupla\n",
    "\n",
    "E' il **prodotto cartesiano** di spazi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coppie in cui il primo elemento è di tipo MultiBinary e il secondo di tipo MultiDiscrete\n",
    "tuple_ = gym.spaces.Tuple((gym.spaces.MultiBinary(5), gym.spaces.MultiDiscrete([7, 9, 3])))\n",
    "print(f\"spazio = {tuple_}\")\n",
    "tuple_.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambienti disponibili"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono registrati nel file `__init__.py` del package `gym/envs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.envs.registry.env_specs\n",
    "print(envs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic\n",
    "\n",
    "L'agente deve programmare delle **macchine di Turing** ad eseguire semplici compiti, come **copiare una stringa**. Ogni macchina ha un nastro di input e uno di output, che possono essere uni o bi-dimensionali. Ad ogni istante l'agente deve deve:\n",
    "* **spostare la testina** del nastro di input in una delle due (o quattro nel caso bidimensionale) direzioni possibili\n",
    "* decidere se vuole **scrivere o meno** sul nastro di **output**\n",
    "* in caso positivo, **selezionare il carattere**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episodes(gym.make('Copy-v0'), sleep_seconds=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari\n",
    "\n",
    "Un **emulatore** dell' [Atari 2600](https://it.wikipedia.org/wiki/Atari_2600) con diversi giochi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episodes(gym.make('SpaceInvaders-v0'), sleep_seconds=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box2D\n",
    "\n",
    "Ambienti che utilizzano la libreria di **fisica bidimensionale** [Box2D](https://box2d.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episodes(gym.make('LunarLander-v2'), sleep_seconds=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic control\n",
    "\n",
    "**Sistemi fisici** estremamente **semplici** come il pendolo, il pendolo doppio e il pendolo inverso (**Cart-Pole**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episodes(gym.make('CartPole-v0'), sleep_seconds=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy text\n",
    "\n",
    "Semplici ambienti con visualizzazione **testuale**.\n",
    "<br>\n",
    "Nel **Frozen Lake** l'agente deve raggiungere la casella G (Goal) evitando le caselle H (Hole)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episodes(gym.make('FrozenLake8x8-v0'), sleep_seconds=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ambiente può essere creato invocando direttamente il **costruttore** della sua classe, oltre che tramite la funzione `gym.make`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.box2d.bipedal_walker import BipedalWalker\n",
    "env = BipedalWalker()\n",
    "generate_episodes(env, sleep_seconds=0.1, max_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il comportamento di un ambiente può essere modificato, o nuove funzionalità possono esseregli aggiunte, utilizzando un oggetto **wrapper** (`gym.core.Wrapper`).\n",
    "<br><br>\n",
    "Possiamo monitorare un addestramento utilizzando il wrapper `Monitor` che salva su disco le **statistiche** e i **video** degli episodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitor import Monitor\n",
    "\n",
    "# salva i file nella sottocartella tmp della cartella corrente\n",
    "env = Monitor(gym.make(\"CartPole-v0\"), directory=\"./tmp\", force=True)\n",
    "generate_episodes(env, sleep_seconds=0, num_episodes=5, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo impostare un numero di step dopo i quali **interrompere l'episodio** anche se non è terminato, utilizzando `TimeLimit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.time_limit import TimeLimit\n",
    "\n",
    "env = TimeLimit(BipedalWalker(), max_episode_steps=10)\n",
    "generate_episodes(env, sleep_seconds=0.1, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo limitare il **range dei reward** ad un determinato intervallo tramite `ClipReward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.clip_reward import ClipReward\n",
    "\n",
    "# limita a 0.5 il massimo reward\n",
    "rewards = []\n",
    "env = ClipReward(gym.make(\"CartPole-v0\"), min_r=0, max_r=0.5)\n",
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    rewards.append(reward)\n",
    "    time.sleep(0.1)\n",
    "env.close()\n",
    "# nel Cart Pole l'agente ottiene un reward di 1 ad ogni passo.\n",
    "# Dopo il clipping i reward valgono tutti 0.5\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono altri wrapper oltre a quelli appena visti (nel package `gym.wrappers`) e naturalmente è possibile crearne di nuovi.\n",
    "<br>\n",
    "È inoltre possibile **comporre** più wrapper.\n",
    "<br>\n",
    "Possiamo ad esempio imporre **contemporaneamente** un limite alla **lunghezza** degli episodi e al **range** dei reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TimeLimit(ClipReward(BipedalWalker(), min_r=0, max_r=0.5), max_episode_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni wrapper espone l'ambiente (o il wrapper) **precedente** con l'attributo `env` e l'ambiente **originario** con l'attributo `unwrapped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env)\n",
    "print(env.env)\n",
    "print(env.unwrapped)\n",
    "# come env.unwrapped\n",
    "print(env.env.env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gli ambienti registrati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere quali sono gli ambienti registrati consultando il **registro globale**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([spec for spec in gym.envs.registry.env_specs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ambiente registrato viene creato tramite la funzione `gym.make`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_v0 = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli ambienti registrati sono gli ambienti **ufficiali** su cui possiamo testare i nostri algortimi e **confrontarli** con quelli degli altri.  [Questa pagina](https://github.com/openai/gym/wiki/Leaderboard) ospita la **classifica** dei migliori algoritmi (in termini di numero di episodi necessari a superare la sfida). Studiarli è un otimo modo per accrescere le proprie competenze di Reinforcement Learning.\n",
    "<br><br>\n",
    "Ad ogni ambiente registrato corrispondono delle **specifiche** (un ogetto della classe `EnvSpec`) accessibili tramite l'attrivuto `spec`.\n",
    "<br><br>\n",
    "Il **CartPole** è presente con le versioni 0 e 1. Nella prima ogni episodio ha una durata massimo di 200 passi (ottenuta con il wrapper TimeLimit). Il compito è considerato **risolto** quando gli ultimi 100 episodi hanno una durata media di almeno 195 passi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"soglia = {}, massimo numero di passi per episodio = {}\".format(cartpole_v0.spec.reward_threshold, \n",
    "                                                                      cartpole_v0.spec.max_episode_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un algoritmo per il pendolo inverso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione ti proponiamo una piccola **sfida**: ideare un algoritmo che risolva il problema del **Cart Pole**.\n",
    "<br>\n",
    "Il Cart Pole è un semplice sistema fisico costituito da un'**asta** imperniata ad un **carrello**, libera di ruotare. È completamente descritto da **quattro parametri**: la posizione del carrello, la sua velocità, l'angolo dell'asta, la sua velocità angolare. L'obiettivo è di tenere **l'asta in equilibrio** senza allontanare troopo il carrello dalla posizione centrale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'occhiata alla **documentazione** della classe `CartPoleEnv` potrebbe esserti utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.envs.classic_control.CartPoleEnv.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `get_action` prende in input uno **stato** e restituisce un'**azione** (per ora scelta a caso). Ti chiediamo di implementarla in modo tale da raggiungere l'obiettivo di una durata media di almeno **195 passi** sui 200 massimi, clacolata su 100 episodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(observation):\n",
    "    <IMPLEMENTA LA FUNZIONE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "lenghts = generate_episodes(env, get_action=get_action, num_episodes=100, render=False)\n",
    "\n",
    "if np.mean(lenghts) >= 195:\n",
    "    print(\"Bravo, hai superato la prova!\")\n",
    "else:\n",
    "    print(f\"La lunghezza media dei 100 episodi è stata di {np.mean(lenghts)}. Devi migliorare!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
