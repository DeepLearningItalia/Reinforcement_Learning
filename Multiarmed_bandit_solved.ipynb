{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HexI5vp__5n7"
   },
   "source": [
    "# Multiarmed bandit\n",
    "\n",
    "Il problema del **multi-armed bandit** è un caso particolare del problema generale del Reinforcement Learning.\n",
    "In esso è presente un solo stato, in cui, ad ogni step, l'agente può scegliere una di $n$ azioni, ciascuna delle quali restituisce un reward da una\n",
    "diversa distribuzione di probabilità. Lo scopo dell'agente è, come al solito, quello di **massimizzare il ritorno totale**.\n",
    "Nel caso del multi-armed bandit è centrale il **dilemma exploration vs exploitation**, poiché l'agente deve sfruttare la sua conoscenza provvisoria dei valori delle azioni per cercare di massimizzare il ritorno (exploitation), ma allo stesso tempo deve continuare ad aggiornarli (exploration).\n",
    "<br><br>\n",
    "Il problema del multi-armed bandit può essere esemplificato da un giocatore d'azzardo che deve cercare di capire quale, di n slot-machine, è più remunerativa. Il problema prende il nome proprio dalle vecchie slot machine, che venivano chiamate, per motivi comprensibili, **one-armed bandit**.\n",
    "\n",
    "<img src=\"img/slot_machines.JPG\" alt=\"slot machines\" style=\"width: 400px;\"/>\n",
    "<center>source: Wikipedia</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9iV-LA4h91zL"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsMgp65y7kXU"
   },
   "outputs": [],
   "source": [
    "class BernoulliMultiarmedBandit:\n",
    "    ''' Rappresenta un multiarmed bandit in cui il braccio i-esimo restituisce\n",
    "    un reward 1 con probabilità p_i e 0 con probabilità 1 - p_i\n",
    "\n",
    "    Attributi\n",
    "    ---------\n",
    "    ps : una lista con i valori di p per ogni braccio'''\n",
    "\n",
    "    def __init__(self, ps):\n",
    "        self.ps = ps\n",
    "\n",
    "    def get_arms_number(self):\n",
    "        ''' Ritorna il numero di braccia del Bandit'''\n",
    "        return len(self.ps)\n",
    "\n",
    "    def pull_arm(self, idx):\n",
    "        ''' Tira un braccio del Bandit\n",
    "\n",
    "        Argomenti\n",
    "        ---------\n",
    "        idx : indice del braccio da tirare\n",
    "\n",
    "        Ritorno\n",
    "        -------\n",
    "        Un reward: 1 o 0'''\n",
    "        return random.choices([1, 0], weights=[self.ps[idx], 1 - self.ps[idx]], k=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfKQTNIk2yyX"
   },
   "source": [
    "# Epsilon-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mj4qqzqWBrXu"
   },
   "source": [
    "Ad ogni step l'agente sceglie l'azione con il valore stimato più alto con probabilità $\\epsilon + \\frac{\\epsilon}{\\left|A\\right|}$ (exploitation), un'altra azione con probabilità $1 - \\epsilon - \\frac{\\epsilon}{\\left|A\\right|}$ (exploration), dove $\\left|A\\right|$ è il numero delle azioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ89FjEsAdZs"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyPlayer:\n",
    "    ''' Giocatore che utilizza la strategia epsilon-greedy\n",
    "\n",
    "    Attributi\n",
    "    ---------\n",
    "    epsilon : il fattore di esplorazione'''\n",
    "\n",
    "    def __init__(self, bandit, epsilon, init_value=0):\n",
    "        '''\n",
    "        Argomenti\n",
    "        ---------\n",
    "        init_values : valori iniziali delle azioni'''\n",
    "\n",
    "        self.bandit = bandit\n",
    "        self.epsilon = epsilon\n",
    "        self.init_values = [init_value] * bandit.get_arms_number()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.actions_means = list(self.init_values)\n",
    "        self.actions_counts = [0] * self.bandit.get_arms_number()\n",
    "\n",
    "    def _get_random_action(self):\n",
    "        ''' Sceglie un'azione a caso'''\n",
    "        return random.randrange(self.bandit.get_arms_number())\n",
    "\n",
    "    def _get_optimal_action(self):\n",
    "        max_value = max(self.actions_means)\n",
    "        # ritorna l'indice della prima tra le azioni che hanno valore massimo\n",
    "        return self.actions_means.index(max_value)\n",
    "\n",
    "    def _update_action_mean_reward(self, action, reward):\n",
    "        ''' Aggiorna il valore di un'azione\n",
    "\n",
    "        Argomenti\n",
    "        ---------\n",
    "        action : indice dell'azione da aggiornare\n",
    "        reward : nuovo reward ottenuto per l'azione\n",
    "        '''\n",
    "        if self.actions_counts[action] == 0:\n",
    "            self.actions_means[action] = reward\n",
    "        else:\n",
    "            self.actions_means[action] = \\\n",
    "                (self.actions_means[action]*self.actions_counts[action] + reward) /\\\n",
    "                (self.actions_counts[action] + 1)\n",
    "\n",
    "        # aggiorna il conteggio dell'azione\n",
    "        self.actions_counts[action] += 1\n",
    "\n",
    "    def _play_one_action(self):\n",
    "        ''' Sceglie un'azione secondo la strategia implementata.\n",
    "        \n",
    "        Ritorno\n",
    "        -------\n",
    "        action : l'azione scelta\n",
    "        reward : il reward ottenuto'''\n",
    "        if random.random() < self.epsilon:\n",
    "            # sceglie un'azione a caso (exploration)\n",
    "            action = self._get_random_action()\n",
    "        else:\n",
    "            # sceglie una delle azioni migliori (exploitation)\n",
    "            action = self._get_optimal_action()\n",
    "\n",
    "        # effettua l'azione scelta\n",
    "        reward = self.bandit.pull_arm(action)\n",
    "\n",
    "        # aggiorna il reward medio dell'azione scelta\n",
    "        self._update_action_mean_reward(action, reward)\n",
    "\n",
    "        return action, reward\n",
    "\n",
    "    def _play_one_game(self, game_length):\n",
    "        ''' Gioca una partita seguendo la strategia implementata\n",
    "        \n",
    "        Parametri\n",
    "        ---------\n",
    "        game_length : la lunghezza del gioco\n",
    "        \n",
    "        Ritorno\n",
    "        -------\n",
    "        actions : una lista con le azioni scelte\n",
    "        rewards : una lista con i reward ottenuti'''\n",
    "        self._reset()\n",
    "        actions, rewards = [], []\n",
    "        for _ in range(game_length):\n",
    "            action, reward = self._play_one_action()\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "        return actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "kJsD2jod-fqr",
    "outputId": "db077c3a-a774-4257-a31d-5e5976c775d5"
   },
   "outputs": [],
   "source": [
    "# numero di partite da giocare per ottenere le statistiche\n",
    "NUM_GAMES = 1_000\n",
    "# lunghezza di ciascuna partita\n",
    "GAME_LENGTH = 200\n",
    "\n",
    "bandit = BernoulliMultiarmedBandit(\n",
    "    ps=[0.2, 0.1, 0.8, 0.6, 0.2, 0.5, 0.3, 0.7, 0.9])\n",
    "\n",
    "\n",
    "stats = defaultdict(lambda: dict(actions=[], rewards=[]))\n",
    "for epsilon in  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    player = EpsilonGreedyPlayer(bandit, epsilon=epsilon)\n",
    "    for i in range(NUM_GAMES):\n",
    "        actions, rewards = player._play_one_game(GAME_LENGTH)\n",
    "        stats[epsilon]['actions'].append(actions)\n",
    "        stats[epsilon]['rewards'].append(rewards)\n",
    "\n",
    "\n",
    "# mostra i reward cumulativi medi\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "epsilons = []\n",
    "mean_total_rewards = []\n",
    "for epsilon in stats.keys():\n",
    "    epsilons.append(epsilon)\n",
    "    all_episodes_rewards = stats[epsilon]['rewards']\n",
    "    all_total_rewards = [sum(one_game_rewards)\n",
    "                         for one_game_rewards in all_episodes_rewards]\n",
    "    mean_total_rewards.append(np.mean(all_total_rewards))\n",
    "\n",
    "ax.plot(epsilons, mean_total_rewards)\n",
    "\n",
    "# mostra i reward medi\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "for epsilon in stats.keys():\n",
    "    all_episodes_rewards = stats[epsilon]['rewards']\n",
    "    mean_rewards = np.mean(np.stack(all_episodes_rewards), axis=0)\n",
    "    ax.plot(mean_rewards, label=f'epsilon={epsilon}', linewidth=0.5, alpha=1.)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZ-eFgBk3Xbn"
   },
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8raQh78XBtF0"
   },
   "source": [
    "Adottando la strategia $\\epsilon$-greedy l'agente esplora con la stessa frequenza azioni con diversi valori stimati e quindi con diversa probabilità di essere ottimali. La strategia **Softmax** risolve questo problema favorendo le azioni più promettenti.\n",
    "<br>\n",
    "La probabilità che l'azione $a_j$ venga scelta è\n",
    "$$P(a_j) = \\frac{e^{\\frac{R(a_j)}{\\tau}}}{\\sum_{i=1}^{\\left|A\\right|}e^\\frac{R(a_i)}{\\tau}}$$\n",
    "<br>\n",
    "$R(a_i)$ è il **valore stimato** dell'azione $i$-esima.\n",
    "<br>\n",
    "Il parametro $\\tau$ è chiamato **temperatura**. All'aumentare di $\\tau$ la distribuzione di probabilità delle diverse azioni tende alla **distribuzione uniforme**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BogUhvf-RLI"
   },
   "outputs": [],
   "source": [
    "class SoftmaxPlayer:\n",
    "    ''' Giocatore che utilizza la strategia Softmax\n",
    "\n",
    "    Attributi\n",
    "    ---------\n",
    "    tau : la temperatura. Al suo aumentare aumenta l'esplorazione e viceversa'''\n",
    "\n",
    "    def __init__(self, bandit, tau, init_value=0):\n",
    "        '''\n",
    "        Argomenti\n",
    "        ---------\n",
    "        init_values : valori iniziali delle azioni'''\n",
    "\n",
    "        self.bandit = bandit\n",
    "        self.tau = tau\n",
    "        self.init_values = [init_value] * bandit.get_arms_number()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.actions_means = list(self.init_values)\n",
    "        self.actions_counts = [0] * self.bandit.get_arms_number()\n",
    "\n",
    "    def _update_action_mean_reward(self, action, reward):\n",
    "        ''' Aggiorna il valore di un'azione\n",
    "\n",
    "        Argomenti\n",
    "        ---------\n",
    "        action : indice dell'azione da aggiornare\n",
    "        reward : nuovo reward ottenuto per l'azione\n",
    "        '''\n",
    "        if self.actions_counts[action] == 0:\n",
    "            self.actions_means[action] = reward\n",
    "        else:\n",
    "            self.actions_means[action] = \\\n",
    "                (self.actions_means[action]*self.actions_counts[action] + reward) /\\\n",
    "                (self.actions_counts[action] + 1)\n",
    "\n",
    "        # aggiorna il conteggio dell'azione\n",
    "        self.actions_counts[action] += 1\n",
    "\n",
    "    def _softmax(self, nums, tau):\n",
    "        nums = np.asarray(nums)\n",
    "        return np.exp(nums / tau).tolist()\n",
    "\n",
    "    def _play_one_action(self):\n",
    "        ''' Sceglie un'azione secondo la strategia implementata.\n",
    "        \n",
    "        Ritorno\n",
    "        -------\n",
    "        action : l'azione scelta\n",
    "        reward : il reward ottenuto'''\n",
    "        weights = self._softmax(self.actions_means, self.tau)\n",
    "        # sceglie una delle azioni pesandole con con i valori restituiti dalla softmax\n",
    "        action = random.choices(\n",
    "            range(self.bandit.get_arms_number()), weights=weights, k=1)[0]\n",
    "        reward = self.bandit.pull_arm(action)\n",
    "        # aggiorna il reward medio dell'azione scelta\n",
    "        self._update_action_mean_reward(action, reward)\n",
    "        return action, reward\n",
    "\n",
    "    def _play_one_game(self, game_length):\n",
    "        ''' Gioca una partita seguendo la strategia implementata\n",
    "        \n",
    "        Parametri\n",
    "        ---------\n",
    "        game_length : la lunghezza del gioco\n",
    "        \n",
    "        Ritorno\n",
    "        -------\n",
    "        actions : una lista con le azioni scelte\n",
    "        rewards : una lista con i reward ottenuti'''\n",
    "        self._reset()\n",
    "        actions, rewards = [], []\n",
    "        for _ in range(game_length):\n",
    "            action, reward = self._play_one_action()\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "        return actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "id": "Hs7YzarE-tVj",
    "outputId": "b5dcde19-9818-4c85-85da-2e1b707584a5"
   },
   "outputs": [],
   "source": [
    "# numero di partite da giocare per ottenere le statistiche\n",
    "NUM_GAMES = 1_000\n",
    "# lunghezza di ciascuna partita\n",
    "GAME_LENGTH = 200\n",
    "\n",
    "bandit = BernoulliMultiarmedBandit(\n",
    "    ps=[0.2, 0.1, 0.8, 0.6, 0.2, 0.5, 0.3, 0.7, 0.9])\n",
    "\n",
    "\n",
    "stats = defaultdict(lambda: dict(actions=[], rewards=[]))\n",
    "for tau in [0.05, 0.08, 0.1, 0.12, 0.15, 0.20]:\n",
    "    player = SoftmaxPlayer(bandit, tau=tau)\n",
    "    for i in range(NUM_GAMES):\n",
    "        actions, rewards = player._play_one_game(GAME_LENGTH)\n",
    "        stats[tau]['actions'].append(actions)\n",
    "        stats[tau]['rewards'].append(rewards)\n",
    "\n",
    "\n",
    "# mostra i reward cumulativi medi\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "taus = []\n",
    "mean_total_rewards = []\n",
    "for tau in stats.keys():\n",
    "    taus.append(tau)\n",
    "    all_episodes_rewards = stats[tau]['rewards']\n",
    "    all_total_rewards = [sum(one_game_rewards)\n",
    "                         for one_game_rewards in all_episodes_rewards]\n",
    "    mean_total_rewards.append(np.mean(all_total_rewards))\n",
    "\n",
    "ax.plot(taus, mean_total_rewards)\n",
    "\n",
    "# mostra i reward medi\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "for tau in stats.keys():\n",
    "    all_episodes_rewards = stats[tau]['rewards']\n",
    "    mean_rewards = np.mean(np.stack(all_episodes_rewards), axis=0)\n",
    "    ax.plot(mean_rewards, label=f'tau={tau}', linewidth=0.5, alpha=1.)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAs_FUbp3ggv"
   },
   "source": [
    "# Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHdelzRcBt6N"
   },
   "source": [
    "Né la strategia $epsilon$-greedy né quella Softmax tengono conto dell'**incertezza** della stima dei valori delle azioni, mentre è ragionevole pensare che - date, ad esempio, due azioni aventi lo stesso valore stimato - l'agente debba favorire l'esplorazione di quella il cui valore è stimato con maggiore incertezza.\n",
    "Con la strategia del Thompson sampling l'agente costruisce e aggiorna continuamente la **distribuzione di probabilità** del valore di ogni azione, ad ogni step estrae un valore da ogni distribuzione ed effettua l'azione corrispondente al valore estratto più alto.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhria5ua-XKm"
   },
   "outputs": [],
   "source": [
    "class ThompsonPlayer:\n",
    "    ''' Giocatore che utilizza il Thompson sampling'''\n",
    "\n",
    "    def __init__(self, bandit):\n",
    "        self.bandit = bandit\n",
    "\n",
    "    def _reset(self):\n",
    "        self.alphas = [1] * self.bandit.get_arms_number()\n",
    "        self.betas = [1] * self.bandit.get_arms_number()\n",
    "\n",
    "    def _update_alphas_betas(self, action, reward):\n",
    "        ''' Aggiorna i valori di alpha e beta per un'azione\n",
    "\n",
    "        Argomenti\n",
    "        ---------\n",
    "        action : indice dell'azione da aggiornare\n",
    "        reward : nuovo reward ottenuto per l'azione\n",
    "        '''\n",
    "        if reward == 1:\n",
    "            self.alphas[action] += 1\n",
    "        else:\n",
    "            self.betas[action] += 1\n",
    "\n",
    "    def _play_one_action(self):\n",
    "        ''' Sceglie un'azione secondo la strategia implementata.\n",
    "        \n",
    "        Ritorno\n",
    "        -------\n",
    "        action : l'azione scelta\n",
    "        reward : il reward ottenuto'''\n",
    "        # estrae dei valori a caso dalle distribuzioni beta associate alle braccia\n",
    "        values = [np.random.beta(alpha, beta) for alpha, beta in zip(self.alphas, self.betas)]\n",
    "        # sceglie l'azione col valore estratto maggiore\n",
    "        action = values.index(max(values))\n",
    "        reward = self.bandit.pull_arm(action)\n",
    "        # aggiorna il reward medio dell'azione scelta\n",
    "        self._update_alphas_betas(action, reward)\n",
    "        return action, reward\n",
    "\n",
    "    def _play_one_game(self, game_length):\n",
    "        ''' Gioca una partita seguendo la strategia implementata\n",
    "        \n",
    "        Parametri\n",
    "        ---------\n",
    "        game_length : la lunghezza del gioco\n",
    "        \n",
    "        Ritorno\n",
    "        -------\n",
    "        actions : una lista con le azioni scelte\n",
    "        rewards : una lista con i reward ottenuti'''\n",
    "        self._reset()\n",
    "        actions, rewards = [], []\n",
    "        for _ in range(game_length):\n",
    "            action, reward = self._play_one_action()\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "        return actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "7zEn-zLk-5rB",
    "outputId": "58194cc0-ee99-4863-a050-9a1abbedf921"
   },
   "outputs": [],
   "source": [
    "# numero di partite da giocare per ottenere le statistiche\n",
    "NUM_GAMES = 1_000\n",
    "# lunghezza di ciascuna partita\n",
    "GAME_LENGTH = 200\n",
    "\n",
    "bandit = BernoulliMultiarmedBandit(\n",
    "    ps=[0.2, 0.1, 0.8, 0.6, 0.2, 0.5, 0.3, 0.7, 0.9])\n",
    "\n",
    "\n",
    "stats = dict(actions=[], rewards=[])\n",
    "player = ThompsonPlayer(bandit)\n",
    "for i in range(NUM_GAMES):\n",
    "    actions, rewards = player._play_one_game(GAME_LENGTH)\n",
    "    stats['actions'].append(actions)\n",
    "    stats['rewards'].append(rewards)\n",
    "\n",
    "\n",
    "# mostra i reward cumulativo medio\n",
    "all_episodes_rewards = stats['rewards']\n",
    "all_total_rewards = [sum(one_game_rewards)\n",
    "                    for one_game_rewards in all_episodes_rewards]\n",
    "print(np.mean(all_total_rewards))\n",
    "\n",
    "# mostra i reward medi\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "all_episodes_rewards = stats['rewards']\n",
    "mean_rewards = np.mean(np.stack(all_episodes_rewards), axis=0)\n",
    "ax.plot(mean_rewards, linewidth=0.5, alpha=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multiarmed_bandit.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
