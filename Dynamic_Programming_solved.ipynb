{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "Consideriamo il MDP in figura, tratto dalla [pagina di Wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process) sui **Markov Decision Process**.\n",
    "\n",
    "![img](img/MDP.png)\n",
    "\n",
    "$S_0, S_1, S_2$ sono gli **stati**.\n",
    "<br>\n",
    "$a_0, a_1$ sono le **azioni**.\n",
    "<br>\n",
    "Le frecce nere rappresentano le possibili **transizioni** con le relative **probabilità**.\n",
    "<br>\n",
    "Le frecce rosse rappresentano i **reward** non nulli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo le **librerie** che useremo nel notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creaimo una classe per rappresentare un **Markov Decision Process**.\n",
    "<br>\n",
    "In particolare \n",
    "* `n_steps` permette di simulare n **transizioni** del processo data una policy\n",
    "<br>\n",
    "* `estimate_state_value` permette di effettuare una **valutazione empirica** del valore degli stati in funzione della policy scelta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mdp:\n",
    "    '''Markov Decision Process Class'''\n",
    "    \n",
    "    def __init__(self, mdp_dict, gamma, initial_state=None, seed=None):\n",
    "        '''\n",
    "        Parametri\n",
    "        ----------\n",
    "        mdp_dict : la struttura del MDP (vedi sotto)\n",
    "        gamma : il discount factor\n",
    "        initial_state : lo stato iniziale. Se è None viene scelto casualmente\n",
    "        seed : è il seme per la generazione dei valori pseudo-casuali\n",
    "        '''\n",
    "        self.mdp_dict = mdp_dict\n",
    "        self.gamma = gamma\n",
    "        self.set_seed(seed)\n",
    "        self.set_state(initial_state)\n",
    "    \n",
    "    def get_states(self):\n",
    "        '''Ritorna tutti gli stati del MDP\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una lista con gli stati\n",
    "        '''\n",
    "        return list(self.mdp_dict.keys())\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        '''Ritorna tutte le azioni possibili di uno stato\n",
    "        \n",
    "        Parametri\n",
    "        ----------        \n",
    "        state : lo stato di cui ritornare le azioni\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una lista con le azioni\n",
    "        '''\n",
    "        return list(self.mdp_dict[state].keys())\n",
    "        \n",
    "    def set_state(self, state=None):\n",
    "        '''Imposta lo stato del MDP\n",
    "        \n",
    "        Parametri\n",
    "        ----------        \n",
    "        state : lo stato da impostare. Se None viene scelto casualmente\n",
    "        '''\n",
    "        if state is None:\n",
    "            # scegle uno stato a caso\n",
    "            self.state = random.choice(self.get_states())\n",
    "        else:\n",
    "            # sceglie lo stato indicato\n",
    "            self.state = state\n",
    "            \n",
    "    def set_seed(self, n):\n",
    "        '''Imposta il seed per la generazione dei numeri casuali\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        n : un intero che rappresenta il seed\n",
    "        '''\n",
    "        random.seed(n)\n",
    "        \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        '''Ritorna la probabilità di una transizione\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : uno stato\n",
    "        action : azione compiuta nello stato state\n",
    "        next_state : stato successivo\n",
    "\n",
    "        Ritorno\n",
    "        ----------\n",
    "        La probabilità di transire nello stato next_state se,\n",
    "        trovandosi nello stato state, si compie l'azione action\n",
    "        '''\n",
    "        if next_state in self.mdp_dict[state][action]:\n",
    "            prob, _ = self.mdp_dict[state][action][next_state]\n",
    "            return prob\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_transition_reward(self, state, action, next_state):\n",
    "        '''Ritorna il reward associato ad una transizione\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : uno stato\n",
    "        action : azione compiuta nello stato state\n",
    "        next_state : stato successivo\n",
    "\n",
    "        Ritorno\n",
    "        ----------\n",
    "        Il reward che si ottiene se, trovandosi nello stato state \n",
    "        e compiendo l'azione action, si transisce in next_state\n",
    "        '''\n",
    "        # NOTE se la transizione ha probabilità 0 che succede?\n",
    "        # La funzione non deve essere chiamata\n",
    "        _, reward = self.mdp_dict[state][action][next_state]\n",
    "        return reward\n",
    "    \n",
    "    # NOTE sicuro che non venga mai chiamato con lo stato non settato?\n",
    "    def step(self, policy):\n",
    "        '''Esegue un'azione a partire dallo stato attuale\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        policy : la policy da seguire\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una tupla composta da:\n",
    "        elemento 1 : lo stato in cui si trova l'MDP dopo l'azione\n",
    "        elemento 2 : il reward ottenuto\n",
    "        '''\n",
    "        # azione scelta\n",
    "        action = policy.get_action(self.state)\n",
    "\n",
    "        # lista dei possibili nuovi stati data l'azione scelta e rispettive probabilità\n",
    "        next_states, states_probs = zip(*[(next_state, prob) for (next_state, (prob, _)) in self.mdp_dict[self.state][action].items()])\n",
    "\n",
    "        # nuovo stato\n",
    "        new_state = random.choices(next_states, weights=states_probs, k=1)[0]\n",
    "        # reward\n",
    "        _, reward = self.mdp_dict[self.state][action][new_state]\n",
    "        # cambia il vecchio stato\n",
    "        self.state = new_state\n",
    "        return new_state, reward\n",
    "        \n",
    "    def n_steps(self, n, policy):\n",
    "        '''Esegue n step a partire dallo stato attuale\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        n : numero di step da effettuare\n",
    "        policy : la policy da seguire\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una tupla composta da:\n",
    "        elemento 1 : una lista degli n stati visitati\n",
    "        elemento 2 : una lista degli n reward ottenuti\n",
    "        '''\n",
    "        states = []\n",
    "        rewards = []\n",
    "        for _ in range(n):\n",
    "            state, reward = self.step(policy)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "        return states, rewards\n",
    "    \n",
    "    def estimate_state_value(self, state, n_steps, n_runs, policy):\n",
    "        '''Esegue una stima del valore dello stato state\n",
    "        \n",
    "        Si eseguono n_steps passi a partire dallo stato state e si ottiene\n",
    "        una stima del valore dello stato sommando i reward ottenuti.\n",
    "        Il processo viene ripetuto n_runs volte e le stime vengono mediate\n",
    "\n",
    "        Parametri\n",
    "        ----------\n",
    "        state : stato di partenza\n",
    "        n_steps : numero di step da effettuare\n",
    "        n_runs : numero di iterazioni del processo\n",
    "        policy : la policy da seguire\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        la media dei ritorni ottenuti\n",
    "        '''\n",
    "        discounts =  [self.gamma**n for n in range(n_steps)]\n",
    "        returns = []\n",
    "        for _ in range(n_runs):\n",
    "            self.set_state(state)\n",
    "            _, rewards = self.n_steps(n_steps, policy)\n",
    "            return_ = sum([reward*discount for (reward, discount) in zip(rewards, discounts)])\n",
    "            returns.append(return_)\n",
    "        return np.mean(returns).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo una classe per rappresentare le **policy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    '''Policy Class'''\n",
    "    \n",
    "    def __init__(self, policy_dict):\n",
    "        '''\n",
    "        Parametri\n",
    "        ----------\n",
    "        policy_dict : un dizionario che rappresenta la policy (vedi sotto)\n",
    "        '''\n",
    "        self.policy_dict = policy_dict\n",
    "        \n",
    "    def get_action_prob(self, state, action):\n",
    "        ''' Ritorna la probabilità di un'azione\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : stato\n",
    "        action: azione\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        La probabilità di compiere l'azione action\n",
    "        nello stato state.\n",
    "        '''\n",
    "        if action in self.policy_dict[state]:\n",
    "            return self.policy_dict[state][action]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''Ritorna un'azione\n",
    "        \n",
    "        Se la policy è stocastica l'azione viene scelta casualmente\n",
    "        in accordo con le probabilità specificate\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : uno stato\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Un'azione\n",
    "        '''\n",
    "        # lista delle azioni possibili nello stato state e rispettive probabilità\n",
    "        actions, action_probs = zip(*[(action, prob) for (action, prob) in self.policy_dict.items()])\n",
    "\n",
    "        actions = list(self.policy_dict[state].keys())\n",
    "        # lista delle rispettive probabilità\n",
    "        action_probs = [self.policy_dict[state][action] for action in actions]\n",
    "        # azione scelta\n",
    "        return random.choices(actions, weights=action_probs, k=1)[0]\n",
    "    \n",
    "    def is_equal_to(self, other_policy):\n",
    "        ''' Stabilisce se la policy è uguale a other_policy\n",
    "\n",
    "        Parametri\n",
    "        ----------\n",
    "        other_policy : la policy da confrontare con la policy self\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        True se le due policy sono uguali, False altrimenti\n",
    "        '''\n",
    "        for state in self.policy_dict.keys():\n",
    "            actions = list(self.policy_dict[state].keys()) + \\\n",
    "                list(other_policy.policy_dict[state].keys())\n",
    "            for action in actions:\n",
    "                if self.get_action_prob(state, action) != other_policy.get_action_prob(state, action):\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dizionario seguente descrive il **Markov Decision Process** illustrato in figura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dizionario con le azioni associate ad ogni stato,\n",
    "# le probabilità di transizione e i reward associati.\n",
    "# Ad esempio se nello stato S2 si sceglie l'azione a1\n",
    "# con probabilità 0.3 si entra nello stato S0 ottenendo\n",
    "# un reward di -1\n",
    "mdp_dict = {\n",
    "    'S0': {\n",
    "        'a0': {'S0': (0.5, 0), 'S2': (0.5, 0)},\n",
    "        'a1': {'S2': (1, 0)}\n",
    "    },\n",
    "    'S1': {\n",
    "        'a0': {'S0': (0.7, +5), 'S1': (0.1, 0), 'S2': (0.2, 0)},\n",
    "        'a1': {'S1': (0.95, 0), 'S2': (0.05, 0)}\n",
    "    },\n",
    "    'S2': {\n",
    "        'a0': {'S0': (0.4, 0), 'S1': (0.6, 0)},\n",
    "        'a1': {'S0': (0.3, -1), 'S1': (0.3, 0), 'S2': (0.4, 0)}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creaimo l'oggetto corrispondente al nostro MDP con un **discount factor** di $0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = Mdp(mdp_dict, gamma=0.8, initial_state=None, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dizionario seguente descrive una **policy stocastica**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una policy stocastica con le probabilità delle varie azioni\n",
    "policy_dict = {\n",
    "    'S0': {'a0': 0.3, 'a1': 0.7},\n",
    "    'S1': {'a0': 0.8, 'a1': 0.2},\n",
    "    'S2': {'a0': 0.5, 'a1': 0.5}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo l'oggetto corrispondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(policy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "source": [
    "La funzione di valore di stato $v_{\\pi }$ può essere approssimata iterando la rispettiva **equazione di Bellman**:\n",
    "<br><br><br>\n",
    "$v_{k+1}(s) = E_{\\pi}\\left [ R_{t+1} + \\gamma v_{k}\\left ( S_{t+1} \\right )| \n",
    " \\, S_{t+1}\\right ] = \\sum_{a}\\pi\\left ( a|s \\right )\\sum_{s', r}p\\left ( s',r|s,a \\right ) \\left [ r+\\gamma v_{k}\\left ( s' \\right ) \\right ]$\n",
    "<br><br><br>\n",
    "La **valutazione iniziale** $v_{0}$ può essere scelta arbitrariamente, ma $v_0(s)=0$ se $s$ è uno **stato terminale** (gli stati terminali non vengono aggiornati).\n",
    "<br> \n",
    "$v_{\\pi}$ è chiaramente un **punto fisso** dell'iterazione precedente e si può dimostrare che se $v_{\\pi}$ esiste l'iterazione converge.\n",
    "N.B. Ricorda di utilizzare due dizionari distinti per rappresentare $v_{k}$ e $v_{k+1}$. In realtà è anche possibile utilizzare un unico dizionario. Anche questa versione **in place** della policy iteration converge."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, epsilon, V=None):\n",
    "    '''Valuta la funzione V (valore di stato) relativa ad una policy\n",
    "    \n",
    "    Parametri\n",
    "    ----------\n",
    "    mdp : il markov decision process a cui applicare la policy\n",
    "    policy : la policy da valutare\n",
    "    epsilon : è la soglia che determina la fine delle iterazioni.\n",
    "              Le iterazioni terminano se max(abs(new_value(S)-old_value(S))) \n",
    "              < epsilon per ogni stato S.\n",
    "    V : è la funzione di valore iniziale. E' un dizionario in cui\n",
    "        le chiavi sono gli stati (stringhe) e i valori i rispettivi valori (float). \n",
    "        Se None la valutazione iniziale assegna il valore 0 ad ogni stato\n",
    "        \n",
    "    Ritorno\n",
    "    ----------\n",
    "    La funzione di valore della policy. E' un dizionario con lo\n",
    "    stesso formato di V\n",
    "    '''\n",
    "    if V is None:\n",
    "        V = {state : 0 for state in mdp.get_states()}\n",
    "    delta = float('inf')\n",
    "    while delta > epsilon:\n",
    "        # crea una copia del dizionario\n",
    "        V_old = V.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.get_states():\n",
    "            # nuovo valore di s\n",
    "            new_vs = 0\n",
    "            for a in mdp.get_actions(s):\n",
    "                acc = 0\n",
    "                for s1 in mdp.get_states():\n",
    "                    # aggiorna V utilizzando V_old\n",
    "                    acc += 0 if mdp.get_transition_prob(s, a, s1) == 0 else \\\n",
    "                        mdp.get_transition_prob(s, a, s1) * (mdp.get_transition_reward(s, a, s1) + mdp.gamma*V_old[s1])\n",
    "                new_vs += policy.get_action_prob(s, a) * acc\n",
    "            V[s] = new_vs\n",
    "            # alla fine del ciclo il delta è pari al massimo aggiornamento che si è avuto\n",
    "            delta = max(delta, abs(V_old[s]-V[s]))\n",
    "    return V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eeguiamo una stima empirica del valore dello stato $S_0$ ottenuta mediando i valori di $4.000$ simulazioni del MDP con stato di partenza $S_0$, ciascuna di $200$ passi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Il valore di S0 è circa 2.140967807665984\n"
     ]
    }
   ],
   "source": [
    "approx_value = mdp.estimate_state_value(\"S0\", n_steps=200, n_runs=4000, policy=policy)\n",
    "print(f\"Il valore di S0 è circa {approx_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontiamo il valore ottenuto con quello ricavato dall'esecuzione della **policy evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'S0': 2.181849290090905, 'S1': 5.30800242662227, 'S2': 2.823569769795702}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "v = policy_evaluation(mdp, policy, epsilon=1e-7); v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa la correttezza della tua implementazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test superato!\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(list(v.values()), [2.182, 5.308, 2.824], atol=1e-3, rtol=1e-3)\n",
    "print(\"Test superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "Dopo avere eseguito la policy iteration per una policy $\\pi$ possiamo utilizzare il valore degli stati per generare una policy migliore $\\pi'$ nel modo seguente:\n",
    "\n",
    "$\n",
    "v_{\\pi'}\\left ( s \\right )=\\underset{a}{\\operatorname{argmax}}\\: q\\left ( s,a \\right )=\\underset{a}{\\operatorname{argmax}}\\: E\\left [ R_{t+1} +\\gamma v_{\\pi}\\left ( S_{t+1} \\right )  | S_t=s, A_t=a \\right]=\\underset{a}{\\operatorname{argmax}}\\sum_{s',r}p\\left ( s',r|s,a \\left [ r+\\gamma v_\\pi\\left ( s' \\right ) \\right ]\\right )\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, V):\n",
    "    '''Ritorna una policy ottimale rispetto alla valutazione V\n",
    "\n",
    "    Parametri\n",
    "    ----------\n",
    "    mdp : un Markov Decision Process\n",
    "    V : una funzione di valutazione di stato\n",
    "    \n",
    "    Ritorno\n",
    "    ----------\n",
    "    Una policy ottimale rispetto a V\n",
    "    '''\n",
    "    # inizializza il dizionario che rappresenta la policy. Tutte le azioni hanno probabilità 0\n",
    "    new_policy_dict = {state : {action : 0 for action in mdp.get_actions(state)} for state in mdp.get_states()}\n",
    "    for s in mdp.get_states():\n",
    "        # rappresenta il valore q(s,a_max) dell'azione migliore nello stato s \n",
    "        # secondo la funzione di valore V\n",
    "        q_a_max = -float('inf')\n",
    "        for a in mdp.get_actions(s):\n",
    "            # è il valore di una specifica azione a compita nello stato s\n",
    "            q_a = 0\n",
    "            # calcola il valore di a\n",
    "            for s1 in mdp.get_states():\n",
    "                q_a += 0 if mdp.get_transition_prob(s, a, s1) == 0 else \\\n",
    "                    mdp.get_transition_prob(s, a, s1)*(mdp.get_transition_reward(s, a, s1)+mdp.gamma*V[s1])\n",
    "            if q_a > q_a_max:\n",
    "                q_a_max = q_a\n",
    "                a_max = a\n",
    "        # la probabilità dell'azione migliore nello stato s viene impostata ad 1\n",
    "        new_policy_dict[s][a_max] = 1\n",
    "    # ritorna una policy deterministica, ottimale rispetto alla funzione di valutazione V\n",
    "    return Policy(new_policy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa la correttezza della tua implementazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test superato!\n"
     ]
    }
   ],
   "source": [
    "policy = policy_improvement(mdp, V={\"S0\":0, \"S1\":5, \"S2\":2})\n",
    "assert policy.is_equal_to(Policy({'S0': {'a0': 0, 'a1': 1}, 'S1': {'a0': 1, 'a1': 0}, 'S2': {'a0': 1, 'a1': 0}}))\n",
    "policy = policy_improvement(mdp, V={\"S0\":10, \"S1\":15, \"S2\":2})\n",
    "assert policy.is_equal_to(Policy({'S0': {'a0': 1, 'a1': 0}, 'S1': {'a0': 0, 'a1': 1}, 'S2': {'a0': 1, 'a1': 0}}))\n",
    "print(\"Test superato!\")"
   ]
  },
  {
   "source": [
    "## Policy Iteration\n",
    "\n",
    "\n",
    "$\n",
    "\\pi_0 \\xrightarrow[]{\\text{evaluation}} v_{\\pi_0} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_1 \\xrightarrow[]{\\text{evaluation}} v_{\\pi_1} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_* \\xrightarrow[]{\\text{evaluation}} v_*\n",
    "$\n",
    "\n",
    "\n",
    "Nella policy iteration si parte da una policy arbitraria $v_0$ e si effettuano alternatamente una policy evaluation e una policy improvement. L'algoritmo termina quando $v_n(S)=v_{n-1}(S)$ per ogni stato $S$. La policy *v_n* è ottimale."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, initial_policy, epsilon):\n",
    "    ''' Esegue una policy iteration\n",
    "    \n",
    "    Parametri\n",
    "    ----------\n",
    "    mdp : un Markov Decision Process\n",
    "    initial_policy : la policy iniziale\n",
    "    epsilon : il valore di soglia per policy evaluation\n",
    "    \n",
    "    Ritorno\n",
    "    ----------\n",
    "    Una tupla composta da:\n",
    "    Elemento 1 : Una policy ottimale per il Markov Decision Process mdp\n",
    "    Elemento 2 : La funzione di valore di stato V della policy ottimale\n",
    "    '''\n",
    "    policy = initial_policy\n",
    "    V = None\n",
    "    while True:\n",
    "        V = policy_evaluation(mdp, policy, epsilon, V)\n",
    "        new_policy = policy_improvement(mdp, V)\n",
    "        if new_policy.is_equal_to(policy):\n",
    "            return (policy, V)\n",
    "        else:\n",
    "            policy = new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy, optimal_values = policy_iteration(mdp=mdp, initial_policy=policy, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test superato!\n"
     ]
    }
   ],
   "source": [
    "assert optimal_policy.is_equal_to(Policy({'S0': {'a0': 0, 'a1': 1}, 'S1': {'a0': 1, 'a1': 0}, 'S2': {'a0': 1, 'a1': 0}}))\n",
    "assert np.allclose(list(optimal_values.values()), [3.423, 6.632, 4.279], atol=1e-3, rtol=1e-3)\n",
    "print(\"Test superato!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}