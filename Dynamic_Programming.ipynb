{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "Consideriamo il MDP in figura, tratto dalla [pagina di Wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process) sui **Markov Decision Process**.\n",
    "\n",
    "![img](img/MDP.png)\n",
    "\n",
    "$S_0, S_1, S_2$ sono gli **stati**.\n",
    "<br>\n",
    "$a_0, a_1$ sono le **azioni**.\n",
    "<br>\n",
    "Le frecce nere rappresentano le possibili **transizioni** con le relative **probabilità**.\n",
    "<br>\n",
    "Le frecce rosse rappresentano i **reward** non nulli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo le **librerie** che useremo nel notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:00.335003Z",
     "start_time": "2020-10-28T15:28:00.319045Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creaimo una classe per rappresentare un **Markov Decision Process**.\n",
    "<br>\n",
    "In particolare \n",
    "* `n_steps` permette di simulare n **transizioni** del processo data una policy\n",
    "<br>\n",
    "* `estimate_state_value` permette di effettuare una **valutazione empirica** del valore degli stati in funzione della policy scelta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:01.113050Z",
     "start_time": "2020-10-28T15:28:01.080138Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mdp:\n",
    "    '''Markov Decision Process Class'''\n",
    "    \n",
    "    def __init__(self, mdp_dict, gamma, initial_state=None, seed=None):\n",
    "        '''\n",
    "        Parametri\n",
    "        ----------\n",
    "        mdp_dict : la struttura del MDP (vedi sotto)\n",
    "        gamma : il discount factor\n",
    "        initial_state : lo stato iniziale. Se è None viene scelto casualmente\n",
    "        seed : è il seme per la generazione dei valori pseudo-casuali\n",
    "        '''\n",
    "        self.mdp_dict = mdp_dict\n",
    "        self.gamma = gamma\n",
    "        self.set_seed(seed)\n",
    "        self.set_state(initial_state)\n",
    "    \n",
    "    def get_states(self):\n",
    "        '''Ritorna tutti gli stati del MDP\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una lista con gli stati\n",
    "        '''\n",
    "        return list(self.mdp_dict.keys())\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        '''Ritorna tutte le azioni possibili di uno stato\n",
    "        \n",
    "        Parametri\n",
    "        ----------        \n",
    "        state : lo stato di cui ritornare le azioni\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una lista con le azioni\n",
    "        '''\n",
    "        return list(self.mdp_dict[state].keys())\n",
    "        \n",
    "    def set_state(self, state=None):\n",
    "        '''Imposta lo stato del MDP\n",
    "        \n",
    "        Parametri\n",
    "        ----------        \n",
    "        state : lo stato da impostare. Se None viene scelto casualmente\n",
    "        '''\n",
    "        if state is None:\n",
    "            # scegle uno stato a caso\n",
    "            self.state = random.choice(self.get_states())\n",
    "        else:\n",
    "            # sceglie lo stato indicato\n",
    "            self.state = state\n",
    "            \n",
    "    def set_seed(self, n):\n",
    "        '''Imposta il seed per la generazione dei numeri casuali\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        n : un intero che rappresenta il seed\n",
    "        '''\n",
    "        random.seed(n)\n",
    "        \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        '''Ritorna la probabilità di una transizione\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : uno stato\n",
    "        action : azione compiuta nello stato state\n",
    "        next_state : stato successivo\n",
    "\n",
    "        Ritorno\n",
    "        ----------\n",
    "        La probabilità di transire nello stato next_state se,\n",
    "        trovandosi nello stato state, si compie l'azione action\n",
    "        '''\n",
    "        if next_state in self.mdp_dict[state][action]:\n",
    "            prob, _ = self.mdp_dict[state][action][next_state]\n",
    "            return prob\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_transition_reward(self, state, action, next_state):\n",
    "        '''Ritorna il reward associato ad una transizione\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : uno stato\n",
    "        action : azione compiuta nello stato state\n",
    "        next_state : stato successivo\n",
    "\n",
    "        Ritorno\n",
    "        ----------\n",
    "        Il reward che si ottiene se, trovandosi nello stato state \n",
    "        e compiendo l'azione action, si transisce in next_state\n",
    "        '''\n",
    "        if next_state in self.mdp_dict[state][action]:\n",
    "            _, reward = self.mdp_dict[state][action][next_state]\n",
    "            return reward\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def step(self, policy):\n",
    "        '''Esegue un'azione a partire dallo stato attuale\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        policy : la policy da seguire\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una tupla composta da:\n",
    "        elemento 1 : lo stato in cui si trova l'MDP dopo l'azione\n",
    "        elemento 2 : il reward ottenuto\n",
    "        '''\n",
    "        # azione scelta\n",
    "        action = policy.get_action(self.state)\n",
    "\n",
    "        # lista dei possibili nuovi stati data l'azione scelta e rispettive probabilità\n",
    "        next_states, states_probs = zip(*[(next_state, prob) for (next_state, (prob, _)) in self.mdp_dict[self.state][action].items()])\n",
    "\n",
    "        # nuovo stato\n",
    "        new_state = random.choices(next_states, weights=states_probs, k=1)[0]\n",
    "        # reward\n",
    "        _, reward = self.mdp_dict[self.state][action][new_state]\n",
    "        # cambia il vecchio stato\n",
    "        self.state = new_state\n",
    "        return new_state, reward\n",
    "        \n",
    "    def n_steps(self, n, policy):\n",
    "        '''Esegue n step a partire dallo stato attuale\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        n : numero di step da effettuare\n",
    "        policy : la policy da seguire\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Una tupla composta da:\n",
    "        elemento 1 : una lista degli n stati visitati\n",
    "        elemento 2 : una lista degli n reward ottenuti\n",
    "        '''\n",
    "        states = []\n",
    "        rewards = []\n",
    "        for _ in range(n):\n",
    "            state, reward = self.step(policy)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "        return states, rewards\n",
    "    \n",
    "    def estimate_state_value(self, state, n_steps, n_runs, policy):\n",
    "        '''Esegue una stima del valore dello stato state\n",
    "        \n",
    "        Si eseguono n_steps passi a partire dallo stato state e si ottiene\n",
    "        una stima del valore dello stato sommando i reward ottenuti.\n",
    "        Il processo viene ripetuto n_runs volte e le stime vengono mediate\n",
    "\n",
    "        Parametri\n",
    "        ----------\n",
    "        state : stato di partenza\n",
    "        n_steps : numero di step da effettuare\n",
    "        n_runs : numero di iterazioni del processo\n",
    "        policy : la policy da seguire\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        la media dei ritorni ottenuti\n",
    "        '''\n",
    "        discounts =  [self.gamma**n for n in range(n_steps)]\n",
    "        returns = []\n",
    "        for _ in range(n_runs):\n",
    "            self.set_state(state)\n",
    "            _, rewards = self.n_steps(n_steps, policy)\n",
    "            return_ = sum([reward*discount for (reward, discount) in zip(rewards, discounts)])\n",
    "            returns.append(return_)\n",
    "        return np.mean(returns).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo una classe per rappresentare le **policy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:02.061385Z",
     "start_time": "2020-10-28T15:28:02.045430Z"
    }
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    '''Policy Class'''\n",
    "    \n",
    "    def __init__(self, policy_dict):\n",
    "        '''\n",
    "        Parametri\n",
    "        ----------\n",
    "        policy_dict : un dizionario che rappresenta la policy (vedi sotto)\n",
    "        '''\n",
    "        self.policy_dict = policy_dict\n",
    "        \n",
    "    def get_action_prob(self, state, action):\n",
    "        ''' Ritorna la probabilità di un'azione\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : stato\n",
    "        action: azione\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        La probabilità di compiere l'azione action\n",
    "        nello stato state.\n",
    "        '''\n",
    "        if action in self.policy_dict[state]:\n",
    "            return self.policy_dict[state][action]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''Ritorna un'azione\n",
    "        \n",
    "        Se la policy è stocastica l'azione viene scelta casualmente\n",
    "        in accordo con le probabilità specificate\n",
    "        \n",
    "        Parametri\n",
    "        ----------\n",
    "        state : uno stato\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        Un'azione\n",
    "        '''\n",
    "        # lista delle azioni possibili nello stato state e rispettive probabilità\n",
    "        actions, action_probs = zip(*[(action, prob) for (action, prob) in self.policy_dict[state].items()])\n",
    "\n",
    "        # azione scelta\n",
    "        return random.choices(actions, weights=action_probs, k=1)[0]\n",
    "    \n",
    "    def is_equal_to(self, other_policy):\n",
    "        ''' Stabilisce se la policy è uguale a other_policy\n",
    "\n",
    "        Parametri\n",
    "        ----------\n",
    "        other_policy : la policy da confrontare con la policy self\n",
    "        \n",
    "        Ritorno\n",
    "        ----------\n",
    "        True se le due policy sono uguali, False altrimenti\n",
    "        '''\n",
    "        for state in self.policy_dict.keys():\n",
    "            actions = set(self.policy_dict[state].keys()).union(set(other_policy.policy_dict[state].keys()))\n",
    "            for action in actions:\n",
    "                if self.get_action_prob(state, action) != other_policy.get_action_prob(state, action):\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dizionario seguente descrive il **Markov Decision Process** illustrato in figura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:02.774570Z",
     "start_time": "2020-10-28T15:28:02.755620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dizionario con le azioni associate ad ogni stato,\n",
    "# le probabilità di transizione e i reward associati.\n",
    "# Ad esempio se nello stato S2 si sceglie l'azione a1\n",
    "# con probabilità 0.3 si entra nello stato S0 ottenendo\n",
    "# un reward di -1\n",
    "mdp_dict = {\n",
    "    'S0': {\n",
    "        'a0': {'S0': (0.5, 0), 'S2': (0.5, 0)},\n",
    "        'a1': {'S2': (1, 0)}\n",
    "    },\n",
    "    'S1': {\n",
    "        'a0': {'S0': (0.7, +5), 'S1': (0.1, 0), 'S2': (0.2, 0)},\n",
    "        'a1': {'S1': (0.95, 0), 'S2': (0.05, 0)}\n",
    "    },\n",
    "    'S2': {\n",
    "        'a0': {'S0': (0.4, 0), 'S1': (0.6, 0)},\n",
    "        'a1': {'S0': (0.3, -1), 'S1': (0.3, 0), 'S2': (0.4, 0)}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creaimo l'oggetto corrispondente al nostro MDP con un **discount factor** di $0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:03.537774Z",
     "start_time": "2020-10-28T15:28:03.522814Z"
    }
   },
   "outputs": [],
   "source": [
    "mdp = Mdp(mdp_dict, gamma=0.8, initial_state=None, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dizionario seguente descrive una **policy stocastica**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:03.928197Z",
     "start_time": "2020-10-28T15:28:03.923212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Una policy stocastica con le probabilità delle varie azioni\n",
    "policy_dict = {\n",
    "    'S0': {'a0': 0.3, 'a1': 0.7},\n",
    "    'S1': {'a0': 0.8, 'a1': 0.2},\n",
    "    'S2': {'a0': 0.5, 'a1': 0.5}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo l'oggetto corrispondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:04.379001Z",
     "start_time": "2020-10-28T15:28:04.361089Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = Policy(policy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione di valore di stato $v_{\\pi }$ può essere approssimata iterando la rispettiva **equazione di Bellman**:\n",
    "<br><br><br>\n",
    "$v_{k+1}(s) = E_{\\pi}\\left [ R_{t+1} + \\gamma v_{k}\\left ( S_{t+1} \\right )| \n",
    " \\, S_{t+1}\\right ] = \\sum_{a}\\pi\\left ( a|s \\right )\\sum_{s', r}p\\left ( s',r|s,a \\right ) \\left [ r+\\gamma v_{k}\\left ( s' \\right ) \\right ]$\n",
    "<br><br><br>\n",
    "La **valutazione iniziale** $v_{0}$ può essere scelta arbitrariamente, ma $v_0(s)=0$ se $s$ è uno **stato terminale** (gli stati terminali non vengono aggiornati).\n",
    "<br> \n",
    "$v_{\\pi}$ è chiaramente un **punto fisso** dell'iterazione precedente e si può dimostrare che se $v_{\\pi}$ esiste l'iterazione converge.\n",
    "N.B. Ricorda di utilizzare due dizionari distinti per rappresentare $v_{k}$ e $v_{k+1}$. In realtà è anche possibile utilizzare un unico dizionario. Anche questa versione **in place** della policy iteration converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:04.953036Z",
     "start_time": "2020-10-28T15:28:04.939073Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, mdp, epsilon, V=None):\n",
    "    '''Valuta la funzione V (valore di stato) relativa ad una policy\n",
    "    \n",
    "    Parametri\n",
    "    ----------\n",
    "    policy : la policy da valutare\n",
    "    mdp : il markov decision process a cui applicare la policy\n",
    "    epsilon : è la soglia che determina la fine delle iterazioni.\n",
    "              Le iterazioni terminano se max(abs(new_value(S)-old_value(S))) \n",
    "              < epsilon per ogni stato S.\n",
    "    V : è la funzione di valore iniziale. E' un dizionario in cui\n",
    "        le chiavi sono gli stati (stringhe) e i valori i rispettivi valori (float). \n",
    "        Se V==None la valutazione iniziale assegna il valore 0 ad ogni stato\n",
    "        \n",
    "    Ritorno\n",
    "    ----------\n",
    "    La funzione di valore della policy. E' un dizionario con lo\n",
    "    stesso formato di V'''\n",
    "    \n",
    "    <SCRIVI QUI IL TUO CODICE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eeguiamo una stima empirica del valore dello stato $S_0$ ottenuta mediando i valori di $4.000$ simulazioni del MDP con stato di partenza $S_0$, ciascuna di $200$ passi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:11.661800Z",
     "start_time": "2020-10-28T15:28:05.724967Z"
    }
   },
   "outputs": [],
   "source": [
    "approx_value = mdp.estimate_state_value(\"S0\", n_steps=200, n_runs=4000, policy=policy)\n",
    "print(f\"Il valore di S0 è circa {approx_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontiamo il valore ottenuto con quello ricavato dall'esecuzione della **policy evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:12.265174Z",
     "start_time": "2020-10-28T15:28:12.236253Z"
    }
   },
   "outputs": [],
   "source": [
    "v = policy_evaluation(policy, mdp, epsilon=1e-7); v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa la correttezza della tua implementazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:13.247309Z",
     "start_time": "2020-10-28T15:28:13.238335Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(list(v.values()), [2.182, 5.308, 2.824], atol=1e-3, rtol=1e-3)\n",
    "print(\"Test superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T14:17:52.429786Z",
     "start_time": "2020-10-28T14:17:52.418817Z"
    },
    "code_folding": []
   },
   "source": [
    "## Policy Improvement\n",
    "Dopo avere eseguito la policy iteration per una policy $\\pi$ possiamo utilizzare il valore degli stati per generare una policy migliore $\\pi'$ nel modo seguente:\n",
    "\n",
    "$v_{\\pi'}\\left ( s \\right )=\\underset{a}{\\operatorname{argmax}}\\: q\\left ( s,a \\right )=\\underset{a}{\\operatorname{argmax}}\\: E\\left [ R_{t+1} +\\gamma v_{\\pi}\\left ( S_{t+1} \\right )  | S_t=s, A_t=a \\right]=\\underset{a}{\\operatorname{argmax}}\\sum_{s',r}p\\left ( s',r|s,a \\left [ r+\\gamma v_\\pi\\left ( s' \\right ) \\right ]\\right )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:14.148356Z",
     "start_time": "2020-10-28T15:28:14.139374Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, V):\n",
    "    '''Ritorna una policy ottimale rispetto alla valutazione V\n",
    "\n",
    "    Parametri\n",
    "    ----------\n",
    "    mdp : un Markov Decision Process\n",
    "    V : una funzione di valutazione di stato\n",
    "    \n",
    "    Ritorno\n",
    "    ----------\n",
    "    Una policy ottimale rispetto a V\n",
    "    '''\n",
    "    \n",
    "    <SCRIVI QUI IL TUO CODICE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:28:14.616315Z",
     "start_time": "2020-10-28T15:28:14.596333Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = policy_improvement(mdp, V={\"S0\":0, \"S1\":5, \"S2\":2})\n",
    "assert policy.is_equal_to(Policy({'S0': {'a0': 0, 'a1': 1}, 'S1': {'a0': 1, 'a1': 0}, 'S2': {'a0': 1, 'a1': 0}}))\n",
    "policy = policy_improvement(mdp, V={\"S0\":10, \"S1\":15, \"S2\":2})\n",
    "assert policy.is_equal_to(Policy({'S0': {'a0': 1, 'a1': 0}, 'S1': {'a0': 0, 'a1': 1}, 'S2': {'a0': 1, 'a1': 0}}))\n",
    "print(\"Test superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "\n",
    "$\\pi_0 \\xrightarrow[]{\\text{evaluation}} v_{\\pi_0} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_1 \\xrightarrow[]{\\text{evaluation}} v_{\\pi_1} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_* \\xrightarrow[]{\\text{evaluation}} v_*$\n",
    "\n",
    "\n",
    "Nella policy iteration si parte da una policy arbitraria $v_0$ e si effettuano alternatamente una policy evaluation e una policy improvement. L'algoritmo termina quando $v_n(S)=v_{n-1}(S)$ per ogni stato $S$. La policy $v_n$ è ottimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:30:42.375160Z",
     "start_time": "2020-10-28T16:30:42.360202Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, initial_policy, epsilon):\n",
    "    ''' Esegue una policy iteration\n",
    "    \n",
    "    Parametri\n",
    "    ----------\n",
    "    mdp : un Markov Decision Process\n",
    "    initial_policy : la policy iniziale\n",
    "    epsilon : il valore di soglia per policy evaluation\n",
    "    \n",
    "    Ritorno\n",
    "    ----------\n",
    "    Una tupla composta da:\n",
    "    Elemento 1 : Una policy ottimale per il Markov Decision Process mdp\n",
    "    Elemento 2 : La funzione di valore di stato V della policy ottimale\n",
    "    '''\n",
    "    \n",
    "    <SCRIVI QUI IL TUO CODICE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:30:50.242694Z",
     "start_time": "2020-10-28T16:30:50.223746Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_policy, optimal_values = policy_iteration(mdp=mdp, initial_policy=policy, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:30:50.750853Z",
     "start_time": "2020-10-28T16:30:50.741877Z"
    }
   },
   "outputs": [],
   "source": [
    "assert optimal_policy.is_equal_to(Policy({'S0': {'a0': 0, 'a1': 1}, 'S1': {'a0': 1, 'a1': 0}, 'S2': {'a0': 1, 'a1': 0}}))\n",
    "assert np.allclose(list(optimal_values.values()), [3.423, 6.632, 4.279], atol=1e-3, rtol=1e-3)\n",
    "print(\"Test superato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:34:42.293713Z",
     "start_time": "2020-10-28T16:34:42.272769Z"
    }
   },
   "source": [
    "## Value iteration\n",
    "\n",
    "Nella value iteration viene iterata l'equazione di Bellman di **$v_*$** (la funzione valore di stato di una policy ottimale).\n",
    "\n",
    "$$v_*(s) = \\max_{a \\, \\in \\, A(s)} q_{\\pi_*}(s, a) = \\max_{a} \\sum_{s', r}p(s', r|s, a)\\left[ r + \\gamma \\, v_*(s')\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:36:37.144285Z",
     "start_time": "2020-10-28T16:36:37.129326Z"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, epsilon, V=None):\n",
    "    ''' Esegue una value iteration\n",
    "    \n",
    "    Parametri\n",
    "    ----------\n",
    "    mdp : un Markov Decision Process\n",
    "    epsilon : valore per la convergenza\n",
    "    V : funzione valore di stato iniziale\n",
    "    \n",
    "    Ritorno\n",
    "    ----------\n",
    "    V : la funzione valore di stato di una policy ottimale\n",
    "    '''\n",
    "   <SCRIVI QUI IL TUO CODICE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:36:37.658394Z",
     "start_time": "2020-10-28T16:36:37.647425Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_values = value_iteration(mdp=mdp, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:36:40.167707Z",
     "start_time": "2020-10-28T16:36:40.152717Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(list(optimal_values.values()), [3.423, 6.632, 4.279], atol=1e-3, rtol=1e-3)\n",
    "print(\"Test superato!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
